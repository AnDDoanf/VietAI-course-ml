{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2Jo5LjHPCwl"
   },
   "source": [
    "# Bài tập về nhà 3: Softmax Regression\n",
    "\n",
    "Trong bài tập này, các bạn sẽ sử dụng kiến thức đã học về softmax regression để giải quyết bài toán phân loại nhiều lớp, cụ thể là 10 lớp (áo, quần, giày dép, ....) từ bộ dữ liệu [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0WnQpFvV1uH"
   },
   "source": [
    "## Giới thiệu\n",
    "Để hoàn tất bài tập này, các bạn cần nắm rõ những kiến thức sau:\n",
    "* Softmax Regression\n",
    "* Cách lấy đạo hàm cho các tham số trong mô hình trên.\n",
    "* Giải thuật gradient descent.\n",
    "\n",
    "Bạn có thể tham khảo lại bài giảng để nắm vững các nội dung này. Ngoài ra, các bạn có thể đặt câu hỏi cho đội ngũ giảng viên nếu có thắc mắc.\n",
    "\n",
    "Bạn cần giải quyết bài tập này bằng cả **numpy** và **Tensorflow**.\n",
    "\n",
    "*Lưu ý: để tiện cho việc phân biệt giữa lớp python và lớp trong bài toán phân loại, người viết quy ước rằng khi viết **class** nghĩa là đang nói về python class, khi viết **lớp** nghĩa là đang nói đến lớp của dữ liệu cần phân loại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beP2NTIcWc2e"
   },
   "source": [
    "## Hướng dẫn nộp bài\n",
    "Ở mỗi bài tập, các bạn sẽ được yêu cầu điền phần còn thiếu vào trong hàm, các cell để thực hiện phần bài làm sẽ có dòng đầu tiên như sau:\n",
    "```python\n",
    "# GRADED FUNCTION: <tên hàm>\n",
    "...\n",
    "```\n",
    "Trong cell đó, các bạn sẽ code phần đáp án của mình giữa 2 phần:\n",
    "```python\n",
    "### START CODE HERE ###\n",
    "<phần bài làm>\n",
    "### END CODE HERE ###\n",
    "```\n",
    "Sau khi thực hiện xong bạn từ terminal, bạn chạy file `submit.py` để nộp bài tập.\n",
    "```\n",
    "python submit.py -filepath <PATH_ĐẾN_FILE_BÀI_LÀM_CỦA_BẠN>\n",
    "```\n",
    "Mặc định `-filepath` là `3_Softmax_Regression.ipynb`. Nếu bạn không thay đổi tên hoặc vị trí file bài tập. Bạn có thể đơn giản gọi dòng lệnh sau để nộp bài:\n",
    "```\n",
    "python submit.py\n",
    "```\n",
    "Sau khi chạy dòng lệnh trên, vui lòng điền `username` và `password`, bạn sẽ nhận được kết quả trả về cho bài làm của bạn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7aYjoAWXu9t"
   },
   "source": [
    "## IMPORT CÁC THƯ VIỆN CẦN THIẾT\n",
    "Nếu chạy trên máy tính cá nhân, trước hết bạn cần install các thư viện cần thiết bằng cách chạy dòng lệnh sau trong terminal:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Nếu bạn chạy trên nền tảng Google Colab, bạn có thể bổ qua bước trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neGnxsHDO8Fu"
   },
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import numpy as np\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DQ_YhCQX32D"
   },
   "source": [
    "## Tải dữ liệu\n",
    "Các bạn chạy cell bên dưới để tải bộ dữ liệu cũng như các hàm dùng để test cách cài đặt của các bạn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jxvIONdhX8tn",
    "outputId": "a658b8a4-f215-4492-cc74-6c0999882e6c"
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "gdd.download_file_from_google_drive(file_id='1umcIzISPX2FhMT3OBg-Io7LninG7Hste', dest_path='./assignment3.zip', unzip=True)\n",
    "!rm assignment3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R168PMadYQFU"
   },
   "source": [
    "Dữ liệu tải xuống của bạn bao gồm:\n",
    "\n",
    "* Folder `fashion-mnist`: chứa 4 zip files dữ liệu.\n",
    "* File softmax_unittest.npy: được dùng để kiểm tra một số hàm mà bạn cài đặt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cqAxrUEiYMcP",
    "outputId": "e347498b-efd5-45d1-bd4b-fc67612a6b99"
   },
   "outputs": [],
   "source": [
    "!ls assignment3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmmCYM_PZBOo"
   },
   "source": [
    "## Các hàm bổ trợ dùng để đọc dữ liệu\n",
    "Nhóm TA sẽ giúp bạn định nghĩa các hàm bổ trợ trong việc đọc dữ liệu, các bạn không cần chỉnh sửa những hàm này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdHIzttSYejU"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These functions help you read data from data files.\n",
    "Author: Kien Huynh\n",
    "\"\"\"\n",
    "\n",
    "def read_mnist_gz(data_path, offset):\n",
    "    with gzip.open(data_path, 'rb') as f:\n",
    "        dataset = np.frombuffer(f.read(), dtype=np.uint8, offset=offset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_mnist_data(sampling_step=20):\n",
    "    print('Reading fashion MNIST data...')\n",
    "    train_x = read_mnist_gz('./assignment3/fashion-mnist/train-images-idx3-ubyte.gz', 16)\n",
    "    train_y = read_mnist_gz('./assignment3/fashion-mnist/train-labels-idx1-ubyte.gz', 8)\n",
    "    test_x = read_mnist_gz('./assignment3/fashion-mnist/t10k-images-idx3-ubyte.gz', 16)\n",
    "    test_y = read_mnist_gz('./assignment3/fashion-mnist/t10k-labels-idx1-ubyte.gz', 8)\n",
    "    num_train = len(train_y)\n",
    "    num_test = len(test_y)\n",
    "\n",
    "    train_x = train_x.reshape((num_train, 28*28))\n",
    "    test_x = test_x.reshape((num_test, 28*28))\n",
    "\n",
    "    val_x = train_x[50000:,:]\n",
    "    val_y = train_y[50000:]\n",
    "    train_x = train_x[:50000,:]\n",
    "    train_y = train_y[:50000]\n",
    "\n",
    "    train_x = train_x[0::sampling_step,:]\n",
    "    train_y = train_y[0::sampling_step]\n",
    "    val_x = val_x[0::sampling_step,:]\n",
    "    val_y = val_y[0::sampling_step]\n",
    "    test_x = test_x[0::sampling_step,:]\n",
    "    test_y = test_y[0::sampling_step]\n",
    "    return train_x.astype(np.float32), train_y, val_x.astype(np.float32), val_y, test_x.astype(np.float32), test_y\n",
    "\n",
    "def get_side(img, side_type, n = 5):\n",
    "    h, w = img.shape\n",
    "    if side_type == \"horizontal\":\n",
    "        return np.ones((h,n))\n",
    "    return np.ones((n,w))\n",
    "\n",
    "def show_gallery(ims,n=5, shuffle=True):\n",
    "    images = []\n",
    "    vertical_images = []\n",
    "    if shuffle:\n",
    "        np.random.shuffle(ims)\n",
    "    vertical_images = []\n",
    "    for i in range(n*n):\n",
    "        img = ims[i].reshape(28,28)\n",
    "        hside = get_side(img,side_type=\"horizontal\")\n",
    "        images.append(img)\n",
    "        images.append(hside)\n",
    "        \n",
    "        if (i+1) % n == 0:\n",
    "            himage=np.hstack((images))\n",
    "            vside = get_side(himage, side_type=\"vertical\")\n",
    "            vertical_images.append(himage)\n",
    "            vertical_images.append(vside)\n",
    "            \n",
    "            images = []\n",
    "        \n",
    "    gallery = np.vstack((vertical_images))\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gallery.astype(np.uint8), cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def add_one(x):\n",
    "    \"\"\"add_one\n",
    "    This function add ones as an additional feature for x.\n",
    "\n",
    "    :param x: input data\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
    "    ### END CODE HERE ###\n",
    "    return x\n",
    "\n",
    "def create_one_hot(labels, num_k=10):\n",
    "    \"\"\"create_one_hot\n",
    "    This function creates a one-hot (one-of-k) matrix based on the given labels\n",
    "\n",
    "    :param labels: list of labels, each label is one of 0, 1, 2,... , num_k - 1\n",
    "    :param num_k: number of classes we want to classify\n",
    "    \"\"\"\n",
    "    eye_mat = np.eye(num_k)\n",
    "    eye_mat = eye_mat[labels, :].astype(np.float32)\n",
    "    return eye_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7faKD0KcExg"
   },
   "source": [
    "## Dữ liệu fashion MNIST\n",
    "\n",
    "Ta có thể đọc tập dữ liệu này bằng hàm `get_mnist_data()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LC6Ak0zJbFao",
    "outputId": "60187d77-1024-4ac1-efeb-17147bb4392c"
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMzhxHDLckdE"
   },
   "source": [
    "Tập dữ liệu này gồm các ảnh xám kích thước $28 \\times 28$. Có tất cả 50000 ảnh train, 10000 ảnh validation và 10000 ảnh test. Mỗi ảnh thuộc một trong 10 loại quần, áo, giày, túi xách, v.v. Tuy nhiên trong bài tập này ta chỉ lấy 2500 ảnh train, 500 ảnh validation và 500 ảnh test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SNkMzye4cJd-",
    "outputId": "09c6f8b2-3b7b-4ab9-b82e-1ecdc501f009"
   },
   "outputs": [],
   "source": [
    "print(\"train_x: \" ,train_x.shape)\n",
    "print(\"train_y: \", train_y.shape)\n",
    "print(\"val_x:   \", val_x.shape)\n",
    "print(\"val_y:   \", val_y.shape)\n",
    "print(\"test_x:  \", test_x.shape)\n",
    "print(\"test_y:  \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IG34oOahc4oB",
    "outputId": "12e5bf4e-dd74-46ca-eb32-30b9118c3f76"
   },
   "outputs": [],
   "source": [
    "show_gallery(train_x, n=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLRR_U2-hiSz"
   },
   "source": [
    "Giá trị của `train_y` và `test_y` có thể là 0, 1, ..., 9 thay vì là 0 và 1 như bài 2 (Logistic Regression). Ngoài ra, dữ liệu ảnh khi được đọc lên sẽ có dạng tensor 2D (3500x784)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wae6pJjxk1H8"
   },
   "source": [
    "## Chuẩn hóa dữ liệu\n",
    "Trong bước này, ta sẽ chuẩn hóa dữ liệu `train_x`, `val_x` và `test_x` theo cách xem các pixel khác nhau trong ảnh là cùng 1 loại đặc trưng (như đã đề cập trong bài tập Logistic Regression).\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{x} = \\frac{1}{mRC}\\sum_{i=0}^{m-1}{\\sum_{r=0}^{R-1}{\\sum_{c=0}^{C-1}{x_{rc}^{(i)}}}} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma=\\sqrt{\\frac{1}{mRC}\\sum_{i=0}^{m-1}{\\sum_{r=0}^{R-1}{\\sum_{c=0}^{C-1}{(x_{rc}^{(i)}-\\overline{x})^2}}}} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Sau khi có được mean và std trên toàn bộ data huấn luyện, ta chuẩn hóa các mẫu trong tập huấn luyện theo cách sau:\n",
    "\n",
    "\\begin{equation}\n",
    "x^{(i)} = \\frac{x^{(i)}-\\overline{x}}{\\sigma} \\tag{5}\n",
    "\\end{equation} \n",
    "\n",
    "Tuy nhiên, do tập dữ liệu MNIST khi load đã được đặt dưới dạng tensor 2D, nên trong công thức tính tổng chỉ còn $m$ và $R=784$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psktAn-TlprH"
   },
   "source": [
    "### TODO 1: normalize (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xw3ozUKnltJj"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalize\n",
    "def normalize(train_x, val_x, test_x):\n",
    "    \"\"\"TODO 1: normalize\n",
    "    This function computes the mean and standard deviation of all pixels and performs data scaling on train_x, val_x and test_x using these computed values.\n",
    "    Note that in this classification problem, the shape of the data is (num_samples, image_width * image_height).\n",
    "\n",
    "    :param train_x: train images, shape=(num_train, image_height * image_width)\n",
    "    :param val_x: validation images, shape=(num_val, image_height * image_width)\n",
    "    :param test_x: test images, shape=(num_test, image_height * image_width)\n",
    "    \"\"\"\n",
    "    # The shape of train_mean and train_std should be (1, 1)\n",
    "    ### START CODE HERE ### (≈5 lines)\n",
    "    train_mean = np.mean(train_x, axis=(0,1), dtype=np.float64, keepdims=True)\n",
    "    train_std = np.std(train_x, axis=(0,1), dtype=np.float64, keepdims=True)\n",
    "\n",
    "    train_x = (train_x-train_mean)/train_std\n",
    "    val_x = (val_x-train_mean)/train_std\n",
    "    test_x = (test_x-train_mean)/train_std\n",
    "    ### END CODE HERE ###\n",
    "    return train_x, val_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6kMQHATlzG9"
   },
   "outputs": [],
   "source": [
    "### SANITY CHECK\n",
    "train_x = np.arange(2*4).reshape(2, 4)\n",
    "x, y, z = normalize(train_x, train_x, train_x)\n",
    "assert np.sum((x, y, z)) == 0, \"Wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yezgW0-3mDjA"
   },
   "source": [
    "## Tiền xử lý vector label thành dạng one-hot\n",
    "\n",
    "Các biến `train_y`, `val_y`, `test_y` lúc này là một vector chứa các giá trị 0, 1, ..., 9; nhưng để tính hàm lỗi của softmax regression, ta nên chuyển chúng về dạng ma trận one-hot (one-of-k). Giả sử ta có vector label có 6 phần tử, mỗi phần tử nằm trong khoảng từ 0 đến 4:\n",
    "\n",
    "\\begin{equation}\n",
    "\ty = [3,4,0,0,2,1]^T \\tag{17}\n",
    "\\end{equation}\n",
    "\n",
    "Ta sẽ có biến đổi one-hot tương ứng của nó là:\n",
    "\\begin{equation}\n",
    "y = \\begin{bmatrix}\n",
    "\t0 & 0 & 0 & \\color{red}1 & 0\\\\\n",
    "\t0 & 0 & 0 & 0 & \\color{red}1\\\\\n",
    "\t\\color{red}1 & 0 & 0 & 0 & 0\\\\\n",
    "\t\\color{red}1 & 0 & 0 & 0 & 0\\\\\n",
    "\t0 & 0 & \\color{red}1 & 0 & 0\\\\\n",
    "\t0 & \\color{red}1 & 0 & 0 & 0\n",
    "\\end{bmatrix} \\tag{18}\n",
    "\\end{equation}\n",
    "\n",
    "Label thứ nhất có giá trị là 3, vì vậy nên trong hàng thứ nhất ở ma trận trên cột 3 có giá trị 1, tất cả các cột khác trong hàng này là 0. Tương tự cho hàng thứ 2, label là 4, nên cột 4 trong hàng 2 có giá trị là 1.\n",
    "\n",
    "\n",
    "Để việc biến đổi giá trị của mảng sang dạng one-hot được nhanh chóng, ta nên sử dụng index array hay index vector trên ma trận đơn vị. Tham khảo thêm ở đây: [Numpy basic indexing - Index arrays](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.indexing.html#index-arrays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0pRUu8vmTGv"
   },
   "source": [
    "## Tính các giá trị phân loại\n",
    "\n",
    "Để tính các giá trị phân loại trong bài này, ta hiện thực các công thức sau trong hàm `feed_forward` và `softmax`:\n",
    "\n",
    "\\begin{equation}\n",
    "z = xw \\space, \\quad x \\in R^{m\\times D}, w \\in R^{D \\times K} \\tag{19}\n",
    "\\end{equation}\n",
    "\n",
    "Trong đó, $m$ là số lượng mẫu dữ liệu, $D$ là số lượng đặc trưng của dữ liệu đầu vào (785 sau khi thêm 1 vào cuối), $K$ là số lượng nhãn trong bài toán ta đang làm (10).\n",
    "\n",
    "\\begin{equation}\n",
    "z_{max} = [max(z^{(0)}), max(z^{(1)}),.., max(z^{(m-1)})]^T  \\tag{20}\n",
    "\\end{equation}\n",
    "\n",
    "Tại đây, $z_{max}$ là một vector cột (kích thước $m \\times 1$).\n",
    "\n",
    "\\begin{equation}\n",
    "z' = e^{z - z_{max}} \\tag{21}\n",
    "\\end{equation}\n",
    "\n",
    "Trong biểu thức trên, ta sẽ dùng cột thứ nhất của $z$ trừ cho $z_{max}$, cột thứ 2 của $z$ trừ cho $z_{max}$, v.v. Sau đó, ta tính lũy thừa cho từng phần tử trên hiệu đã tính. Kết quả tại bước này là một ma trận $z'$ có kích thước $m \\times K$.\n",
    "\n",
    "Kế tiếp, ta cần tính tổng sau:\n",
    "\n",
    "\\begin{equation}\n",
    "s = \\sum^{K-1}_{k=0}z'^{(i)}_{k} ,\\quad 0\\le i \\le m-1\\tag{22}\n",
    "\\end{equation}\n",
    "\n",
    "Như vậy, $s$ sẽ là vector chứa tổng từng hàng của ma trận $z'$. $s$ có kích thước $m \\times 1$. Sau cùng, ta có thể tính softmax bằng cách lấy mỗi phần tử trong $z'$ chia cho tổng hàng tương ứng:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}^{(i)}_{k} = \\frac{z'^{(i)}_{k}}{s^{(i)}}, \\quad 0\\le i \\le m-1, 0\\le k \\le K-1 \\tag{23}\n",
    "\\end{equation}\n",
    "\n",
    "Sau khi tổng hợp lại toàn bộ các phần tử $i, k$ của $\\hat{y}$, ta sẽ có ma trận có kích thước $m \\times K$. Trong ma trận này, mỗi hàng thứ $i$ biểu diễn vector xác suất lớp của mẫu ảnh thứ $i$. Vì vậy, tổng của mỗi hàng luôn bằng 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHmpJu5xiMWD"
   },
   "source": [
    "## Class SoftmaxClassifier với numpy\n",
    "\n",
    "Tương tự như class `LogisticClassifier` đã cài đặt ở bài trước, class SoftmaxClassifier cần được khởi tạo trọng số `w` và có thể sử dụng lại hàm `update_weight` của class `LogisticClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSDXPiuWfgxW"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "class SoftmaxClassifier(object):\n",
    "    def __init__(self, w_shape):\n",
    "        \"\"\"__init__\n",
    "        \n",
    "        :param w_shape: create w with shape w_shape using normal distribution\n",
    "        \"\"\"\n",
    "        self.w = np.random.normal(0, np.sqrt(2./np.sum(w_shape)), w_shape)\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"TODO 2: softmax\n",
    "\n",
    "        :param x: input\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        ### START CODE HERE ### (≈4 lines)\n",
    "        max_x = np.max(x, axis=1, keepdims=True)\n",
    "        x -= max_x\n",
    "        x_exp = np.exp(x)\n",
    "        result = x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "        ### END CODE HERE ###\n",
    "        return result\n",
    "\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"TODO 3: feed_forward\n",
    "        This function computes the output of your softmax regression model\n",
    "        \n",
    "        :param x: input\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        ### START CODE HERE ### (≈2 lines)\n",
    "        x_out = np.dot(x, self.w)\n",
    "        result = self.softmax(x_out)\n",
    "        ### END CODE HERE ###\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        \"\"\"TODO 4: compute_loss\n",
    "        Compute the loss using y (label) and y_hat (predicted class)\n",
    "\n",
    "        :param y:  the label, the actual class of the sample data\n",
    "        :param y_hat: the classifying probabilities of all sample data\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        ### START CODE HERE ### (≈3 lines)\n",
    "        loss = -np.log(y_hat) \n",
    "        loss = np.sum(loss*y, axis=1)\n",
    "        loss = np.mean(loss)\n",
    "        ### END CODE HERE ###\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def get_grad(self, x, y, y_hat):\n",
    "        \"\"\"TODO 5: get_grad\n",
    "        Compute and return the gradient of w\n",
    "\n",
    "        :param loss: computed loss between y_hat and y in the train dataset\n",
    "        :param y_hat: predicted y\n",
    "        \"\"\" \n",
    "        w_grad = None\n",
    "        ### START CODE HERE ### (≈2 lines)\n",
    "        num_x = x.shape[0]\n",
    "        w_grad = np.dot(x.T, -(y - y_hat))/num_x\n",
    "        ### END CODE HERE ###\n",
    "        return w_grad\n",
    "   \n",
    "\n",
    "    def update_weight(self, grad, learning_rate):\n",
    "        \"\"\"update_weight\n",
    "        Update w using the computed gradient.\n",
    "\n",
    "        :param grad: gradient computed from the loss\n",
    "        :param learning_rate: float, learning rate\n",
    "        \"\"\"\n",
    "        self.w = self.w - learning_rate*grad \n",
    "        return self.w\n",
    "\n",
    "\n",
    "    def update_weight_momentum(self, grad, learning_rate, momentum, momentum_rate):\n",
    "        \"\"\"update_weight using momentum\n",
    "        BONUS:[YC1.8]\n",
    "        Update w using the algorithm with momentum\n",
    "\n",
    "        :param grad: gradient computed from the loss\n",
    "        :param learning_rate: float, learning rate\n",
    "        :param momentum: the array storing momentum for training w, should have the same shape as that of w\n",
    "        :param momentum_rate: float, how much momentum to reuse after each loop (denoted as gamma in the following section)\n",
    "        \"\"\"\n",
    "        momentum *= momentum_rate\n",
    "        momentum += learning_rate*grad\n",
    "        self.w = self.w - momentum\n",
    "        return self.w\n",
    "    \n",
    "    def numerical_check(self, x, y, grad):\n",
    "        i = 3\n",
    "        j = 0\n",
    "        eps = 0.000005\n",
    "        w_test0 = np.copy(self.w)\n",
    "        w_test1 = np.copy(self.w)\n",
    "        w_test0[i,j] = w_test0[i,j] - eps\n",
    "        w_test1[i,j] = w_test1[i,j] + eps\n",
    "\n",
    "        y_hat0 = np.dot(x, w_test0)\n",
    "        y_hat0 = self.softmax(y_hat0)\n",
    "        loss0 = self.compute_loss(y, y_hat0) \n",
    "\n",
    "        y_hat1 = np.dot(x, w_test1)\n",
    "        y_hat1 = self.softmax(y_hat1)\n",
    "        loss1 = self.compute_loss(y, y_hat1) \n",
    "\n",
    "        numerical_grad = (loss1 - loss0)/(2*eps)\n",
    "        print(numerical_grad)\n",
    "        print(grad[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBaqaYrjinaY"
   },
   "outputs": [],
   "source": [
    "|### SANITY CHECK\n",
    "eps = 0.001        \n",
    "classifier = SoftmaxClassifier((3,1))\n",
    "classifier.w = np.arange(3*1).reshape(3,1)\n",
    "x = np.ones(2*3).reshape(2,3)/2\n",
    "y = np.zeros([2,1])\n",
    "y_hat = classifier.feed_forward(x)\n",
    "assert sum(y_hat)==2, \"Wrong\"\n",
    "loss = classifier.compute_loss(y, y_hat)\n",
    "assert loss==0, \"Wrong\"\n",
    "grad = classifier.get_grad(x, y, y_hat)\n",
    "assert sum(grad)==1.5, \"Wrong\"\n",
    "updateweight = classifier.update_weight(grad, 0.1)\n",
    "assert sum(updateweight) - 2.849 < eps, \"Wrong\"\n",
    "updatemomen = classifier.update_weight_momentum(grad, 0.1, 0.1, 0.1)\n",
    "assert sum(updatemomen) - 2.67 < eps, \"Wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "by39evlbmaA_"
   },
   "source": [
    "### TODO 2: feed_forward (10)\n",
    "Các bạn hoàn thành hàm `feed_forward` ở phần class SoftmaxClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NaAn4ixmzWs"
   },
   "source": [
    "### TODO 3: softmax (10)\n",
    "Các bạn hoàn thành hàm `softmax` ở phần class SoftmaxClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gD5HWyPsm2Kj"
   },
   "source": [
    "### Tính độ lỗi\n",
    "Công thức tính độ lỗi category như sau:\n",
    "\n",
    "\\begin{equation}\n",
    "J(w) = -\\frac{1}{m}\\sum_{i=0}^{m-1}\\sum_{k=0}^{K-1} y^{(i)}_k \\log\\hat{y}^{(i)}_k \\tag{24}\n",
    "\\end{equation}\n",
    "\n",
    "Trong đó:\n",
    "- $y^{(i)}_k$ là phần tử hàng $i$ cột $k$ trong ma trận nhãn one-hot $y$ (đã đề cập trong phần trước).\n",
    "- $\\hat{y}^{(i)}_k \\in (0, 1)$ là hàng $i$ cột $k$ trong ma trận $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBeo57MKuAm_"
   },
   "source": [
    "#### TODO 4: compute_loss (10)\n",
    "Các bạn hoàn thành hàm `compute_loss` ở phần class SoftmaxClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2JO5yQunwNB"
   },
   "source": [
    "## Tính đạo hàm\n",
    "Công thức tính đạo hàm theo từng tham số $w_{jk}$ được biểu diễn như sau:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial  J(w_{jk})}{\\partial w_{jk}} = \\frac{1}{m}\\sum_{i=0}^{m-1} x^{(i)}_j (\\hat{y}^{(i)}_k - y^{(i)}_k) \\tag{25}\n",
    "\\end{equation}\n",
    "\n",
    "Với $0 \\le j \\le D-1$. Viết lại dưới dạng ma trận ta có được:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial  J(w)}{\\partial w} = \\frac{1}{m}x^T(\\hat{y} - y) \\tag{26}\n",
    "\\end{equation}\n",
    "\n",
    "### TODO 5: get_grad (10)\n",
    "Các bạn hoàn thành hàm `get_grad` ở phần class SoftmaxClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhI9LGbEoGYo"
   },
   "source": [
    "## Đề xuất điều kiện dùng vòng lặp huấn luyện\n",
    "\n",
    "Trong bài này, ngoài việc sử dụng `train_x`, `train_y` để huấn luyện, file mẫu còn cung cấp cho các bạn các biến `val_x`, `val_y` để thực hiện việc validation. Toàn bộ các giá trị lỗi validation được lưu trong biến `all_val_loss`. Trong phần này, nhiệm vụ của các bạn là đề xuất ra điều kiện dừng khác (ngoài việc `e` đạt `num_epoch`) trong quá trình huấn luyện. Dựa vào độ lỗi validation, bạn có thể sử dụng các tiêu chí mình tự đề ra để tránh việc quá trình huấn luyện bị overfitting.\n",
    "\n",
    "### TODO 6: is_stop_training (20)\n",
    "Các bạn hoàn thành hàm `is_stop_training` nhận vào mảng `all_val_loss` và trả về giá trị boolean True hoặc False quyết định có dừng việc huấn luyện không. Nếu loss trên tập validation tăng ít nhất n (patience) lần liên tiếp thì trả về True, ngược lại thì False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCnG7E4Ifw4j"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def is_stop_training(all_val_loss, patience=5):\n",
    "    \"\"\"TODO 6:  is_stop_training\n",
    "    Check whether training need to be stopped\n",
    "\n",
    "    :param all_val_loss: list of all validation loss values during training\n",
    "    \"\"\"\n",
    "    is_stopped = False\n",
    "    ### START CODE HERE ###\n",
    "    num_val_increase = 0\n",
    "    if (len(all_val_loss) < patience+1):\n",
    "        return False\n",
    "    \n",
    "    for i in range(len(all_val_loss) - 1):\n",
    "        if all_val_loss[i] < all_val_loss[i+1]:\n",
    "            num_val_increase += 1\n",
    "        else:\n",
    "            num_val_increase = 0\n",
    "        if num_val_increase >= patience:\n",
    "            return True\n",
    "    ### END CODE HERE ###\n",
    "    return is_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KN4ZrrNbofm8"
   },
   "outputs": [],
   "source": [
    "### SANITY CHECK\n",
    "assert is_stop_training([1, 2, 3, 4, 5, 6]) == True, \"Wrong\"\n",
    "assert is_stop_training([1, 2, 3]) == False, \"Wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHSHuB-LokPm"
   },
   "source": [
    "## Đánh giá mô hình trên dữ liệu test\n",
    "\n",
    "Để đánh giá mô hình phân loại nhiều lớp, ta cần sử dụng một khái niệm gọi là confusion matrix. Giả sử ta có bài toán phân loại 3 lớp, thì confusion matrix có dạng sau:\n",
    "\n",
    "<table>\n",
    "    <th>\n",
    "        <td>Lớp 1</td>\n",
    "        <td>Lớp 2</td>\n",
    "        <td>Lớp 3</td>\n",
    "    </th>\n",
    "    <tr>\n",
    "        <td>Lớp 1</td>\n",
    "        <td>$N_{11}$</td>\n",
    "        <td>$N_{12}$</td>\n",
    "        <td>$N_{13}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Lớp 2</td>\n",
    "        <td>$N_{21}$</td>\n",
    "        <td>$N_{22}$</td>\n",
    "        <td>$N_{23}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Lớp 3</td>\n",
    "        <td>$N_{31}$</td>\n",
    "        <td>$N_{32}$</td>\n",
    "        <td>$N_{33}$</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Trong đó, $N_{kl}$ là tổng số mẫu thực chất thuộc lớp $k$ và bộ phân loại phân loại thành lớp $l$ chia cho tổng số mẫu trong lớp k. Bộ phân loại càng tốt thì các ô trên đường chéo chính sẽ càng cao hơn so với các ô xung quanh. Khi tiến hành kiểm thử, người ra đề đã tính được confusion matrix như sau:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>0.93</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.05</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0.95</td>\n",
    "        <td>0</td>\n",
    "        <td>0.05</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.02</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0.59</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0.22</td>\n",
    "        <td>0</td>\n",
    "        <td>0.13</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.05</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.82</td>\n",
    "        <td>0.05</td>\n",
    "        <td>0</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.07</td>\n",
    "        <td>0.05</td>\n",
    "        <td>0.77</td>\n",
    "        <td>0</td>\n",
    "        <td>0.12</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.89</td>\n",
    "        <td>0</td>\n",
    "        <td>0.09</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.21</td>\n",
    "        <td>0</td>\n",
    "        <td>0.08</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0.17</td>\n",
    "        <td>0</td>\n",
    "        <td>0.51</td>\n",
    "        <td>0</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.04</td>\n",
    "        <td>0</td>\n",
    "        <td>0.86</td>\n",
    "        <td>0</td>\n",
    "        <td>0.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0</td>\n",
    "        <td>0.04</td>\n",
    "        <td>0</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0.93</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.02</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.98</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Bạn cần đạt được kết quả tương tự hoặc tốt hơn sau khi hoàn tất bài tập 2 này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWTxRxc-ouFq"
   },
   "source": [
    "### TODO 7: softmax_test (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_NDwfLpohfW"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def softmax_test(y_hat, test_y):\n",
    "    \"\"\"TODO 7: test\n",
    "    Compute the confusion matrix based on labels and predicted values \n",
    "\n",
    "    :param classifier: the trained classifier\n",
    "    :param y_hat: predicted probabilites, output of classifier.feed_forward\n",
    "    :param test_y: test labels\n",
    "    \"\"\"\n",
    "\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "    test_y = np.argmax(test_y, axis=1)\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(10):\n",
    "        class_i_idx = test_y == i\n",
    "        num_class_i = np.sum(class_i_idx)\n",
    "        y_hat_i = y_hat[class_i_idx]\n",
    "        for j in range(10):\n",
    "            confusion_mat[i,j] = 1.0*np.sum(y_hat_i == j)/num_class_i\n",
    "    ### END CODE HERE ###\n",
    "    return confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kgp4cAjEo7PM"
   },
   "outputs": [],
   "source": [
    "y_hat = np.eye(20, 10)\n",
    "test_y = y_hat > 0.5\n",
    "assert np.trace(softmax_test(y_hat, test_y)) == 10, \"Wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mP8dZfG0o_pP"
   },
   "source": [
    "## Vòng lặp huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhINMd-tpCb4"
   },
   "outputs": [],
   "source": [
    "def plot_softmax_loss(train_loss, val_loss):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.plot(train_loss, color='b')\n",
    "    plt.plot(val_loss, color='g')\n",
    "\n",
    "\n",
    "def draw_weight(w):\n",
    "    label_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    plt.figure(2, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    w = w[0:(28*28),:].reshape(28, 28, 10)\n",
    "    for i in range(10):\n",
    "        ax = plt.subplot(3, 4, i+1)\n",
    "        plt.imshow(w[:,:,i], interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        ax.set_title(label_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "colab_type": "code",
    "id": "JJu8r-owo70a",
    "outputId": "33dd560d-6a21-4428-b9ed-25fa768ee7a4"
   },
   "outputs": [],
   "source": [
    "#@title Training { display-mode: \"both\" }\n",
    "num_epoch = 10000 #@param {type:\"integer\"}\n",
    "learning_rate = 0.01 #@param {type:\"number\"}\n",
    "momentum_rate = 0.9 #@param {type:\"number\"}\n",
    "epochs_to_draw = 10 #@param {type:\"integer\"}\n",
    "update_weight_method = \"normal\" #@param [\"normal\", \"momentum\"]\n",
    "\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Load data from file\n",
    "# Make sure that fashion-mnist/*.gz files is in data/\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n",
    "num_train = train_x.shape[0]\n",
    "num_val = val_x.shape[0]\n",
    "num_test = test_x.shape[0]  \n",
    "\n",
    "# Convert label lists to one-hot (one-of-k) encoding\n",
    "train_y = create_one_hot(train_y)\n",
    "val_y = create_one_hot(val_y)\n",
    "test_y = create_one_hot(test_y)\n",
    "\n",
    "# Normalize our data\n",
    "train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n",
    "\n",
    "# Pad 1 as the last feature of train_x and test_x\n",
    "train_x = add_one(train_x) \n",
    "val_x = add_one(val_x)\n",
    "test_x = add_one(test_x)\n",
    "\n",
    "# Create classifier\n",
    "num_feature = train_x.shape[1]\n",
    "dec_classifier = SoftmaxClassifier((num_feature, 10))\n",
    "momentum = np.zeros_like(dec_classifier.w)\n",
    "\n",
    "# Define hyper-parameters and train-related parameters\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "plt.ion()\n",
    "\n",
    "for e in range(num_epoch):    \n",
    "    train_y_hat = dec_classifier.feed_forward(train_x)\n",
    "    val_y_hat = dec_classifier.feed_forward(val_x)\n",
    "\n",
    "    train_loss = dec_classifier.compute_loss(train_y, train_y_hat)\n",
    "    val_loss = dec_classifier.compute_loss(val_y, val_y_hat)\n",
    "\n",
    "    grad = dec_classifier.get_grad(train_x, train_y, train_y_hat)\n",
    "\n",
    "    # dec_classifier.numerical_check(train_x, train_y, grad)\n",
    "    # Updating weight: choose either normal GD or GD with momentum\n",
    "    if update_weight_method == \"normal\":\n",
    "        dec_classifier.update_weight(grad, learning_rate)\n",
    "    else:\n",
    "        dec_classifier.update_weight_momentum(grad, learning_rate, momentum, momentum_rate)\n",
    "\n",
    "    all_train_loss.append(train_loss) \n",
    "    all_val_loss.append(val_loss)\n",
    "    \n",
    "    if is_stop_training(all_val_loss):\n",
    "        break\n",
    "\n",
    "    if (e % epochs_to_draw == epochs_to_draw-1):\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=True)\n",
    "        plot_softmax_loss(all_train_loss, all_val_loss)\n",
    "        draw_weight(dec_classifier.w)\n",
    "        plt.show()\n",
    "        plt.pause(0.1) \n",
    "        print(\"Epoch %d: train loss: %.5f || val loss: %.5f\" % (e+1, train_loss, val_loss))\n",
    "\n",
    "y_hat = dec_classifier.feed_forward(test_x)\n",
    "np.set_printoptions(precision=2)\n",
    "confusion_mat = softmax_test(y_hat, test_y)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_mat)\n",
    "print('Diagonal values:')\n",
    "print(confusion_mat.flatten()[0::11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNw0WgOopprp"
   },
   "source": [
    "## Class SoftmaxRegressionTF bằng Tensorflow\n",
    "\n",
    "Tại đây, các bạn cần định nghĩa mô hình SoftmaxRegressionTF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uforIT1NpG08"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "class SoftmaxRegressionTF(tf.keras.Model):\n",
    "    def __init__(self, num_class):\n",
    "        super(SoftmaxRegressionTF, self).__init__()\n",
    "        # TODO 8: init all weights \n",
    "        ### START CODE HERE ###\n",
    "        self.dense = tf.keras.layers.Dense(num_class, kernel_initializer=tf.keras.initializers.RandomNormal(seed=2020))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # TODO 9: implement your feedforward \n",
    "        ### START CODE HERE ###\n",
    "        output = self.dense(inputs)\n",
    "        ### END CODE HERE ###\n",
    "        try:\n",
    "          output = tf.nn.softmax(output)\n",
    "        except: # if softmax op does not exist on the gpu\n",
    "          with tf.device('/cpu:0'):\n",
    "              output = tf.nn.softmax(output) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YCYLggEQqXNX",
    "outputId": "234a4bdc-0f2e-4829-fb65-9c94a3da334c"
   },
   "outputs": [],
   "source": [
    "### SANITY CHECK\n",
    "logistic_regressor = SoftmaxRegressionTF(10)\n",
    "dummy_x = tf.zeros((1, 13))\n",
    "assert logistic_regressor(dummy_x).numpy().sum() == 1, \"Wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5u2S72X_XQrp",
    "outputId": "42793e1f-f055-4666-8762-8b823d1fd22a"
   },
   "outputs": [],
   "source": [
    "print((logistic_regressor.weights[0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Fbjy5EMqbol"
   },
   "source": [
    "### TODO 8: init (10) + call (10)\n",
    "Các bạn cần khởi tạo tất cả các trọng số ở hàm `init` và `call` của lớp `SoftmaxRegressionTF`. Để tiện cho việc chấm bài, các bạn vui lòng set `kernel_initializer=tf.keras.initializers.RandomNormal(seed=2020)` khi init weights. Nếu bạn không cài đặt `kernel_initializer` như yêu cầu, bạn sẽ bị mất điểm cho TODO 8 này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkX_B6GNrbrl"
   },
   "source": [
    "## Huấn luyện với tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6yiZR-QvrRC2",
    "outputId": "1b784930-1949-4d9e-92f6-2d4708ab6f32"
   },
   "outputs": [],
   "source": [
    "#@title Training { display-mode: \"both\" }\n",
    "num_epoch = 100 #@param {type:\"integer\"}\n",
    "learning_rate = 0.001 #@param {type:\"number\"}\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "tf.random.set_seed(2020)\n",
    "\n",
    "# Load data from file\n",
    "# Make sure that fashion-mnist/*.gz files is in data/\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n",
    "num_train = train_x.shape[0]\n",
    "num_val = val_x.shape[0]\n",
    "num_test = test_x.shape[0]  \n",
    "\n",
    "\n",
    "# Convert label lists to one-hot (one-of-k) encoding\n",
    "train_y = create_one_hot(train_y)\n",
    "val_y = create_one_hot(val_y)\n",
    "test_y = create_one_hot(test_y)\n",
    "\n",
    "# Normalize our data\n",
    "train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n",
    "\n",
    "device = '/cpu:0' if len(tf.config.experimental.list_physical_devices('GPU')) == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = SoftmaxRegressionTF(num_classes)\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    # train\n",
    "    model.fit(train_x, train_y, batch_size=batch_size, epochs=num_epoch,\n",
    "              validation_data=(val_x, val_y), verbose=2)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(test_x, test_y, 32, verbose=2)\n",
    "    \n",
    "    y_hat = model.predict(test_x)\n",
    "\n",
    "\n",
    "    confusion_mat = softmax_test(y_hat, test_y)\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_mat)\n",
    "    print('Diagonal values:')\n",
    "    print(confusion_mat.flatten()[0::11])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_Softmax_Regression_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
