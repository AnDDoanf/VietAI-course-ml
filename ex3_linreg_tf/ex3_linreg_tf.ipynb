{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Mô Hình Linear Regression với Tensorflow\n",
    "### Tổng quan\n",
    "\n",
    "Tiếp nối phần bài tập trước, ở phần bài này chúng ta sẽ học cách sử dụng thư viện **Tensorflow** để xây dựng **Mô hình Linear Regression**.\n",
    "\n",
    "<div style=\"margin-left: auto;margin-right: auto;width: 60%\" ><img src=\"./imgs/tensorflow.png\" /></div>\n",
    "\n",
    " \n",
    "### Mục tiêu học tập\n",
    "\n",
    "* Học cách sử dụng thư viện **Tensorflow**. \n",
    "* Có thể viết các hàm đơn giản với **Tensorflow** để giải quyết **bài toán Hồi quy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hướng dẫn làm và nộp bài\n",
    "1./ Ở mỗi bài tập, các bạn sẽ được yêu cầu điền phần còn thiếu vào trong hàm. Các cell cần điền sẽ có dòng đầu tiên như sau:\n",
    "```python\n",
    "# GRADED FUNCTION: <tên hàm>\n",
    "...\n",
    "```\n",
    "Trong cell đó, các bạn sẽ code phần đáp án của mình giữa 2 phần:\n",
    "```python\n",
    "### START CODE HERE ###\n",
    "<phần bài làm>\n",
    "### END CODE HERE ###\n",
    "```\n",
    "Sau đó, các bạn trả về giá trị đầu ra của hàm số (nếu có) sau phần `return` (ví du: `return output`, bỏ giá trị **None** đi)\n",
    "\n",
    "2./ Để kiểm tra lỗi nhanh cho hàm bạn vừa viết, thực hiện chạy phần `Sanity check` ở sau mỗi TO_DO.  \n",
    "\n",
    "3./ Tiếp theo, sau khi đã vượt qua Sanity check, bạn chạy file `check_score.py` để kiểm tra điểm mà bạn hiện tại cho phần bài làm của bạn. Nếu không thay đổi tên và vị trí mặc định của file bài tập, bạn có thể gọi dòng lệnh sau từ terminal để nộp bài:\n",
    "```\n",
    "python check_score.py\n",
    "```\n",
    "Nếu bạn đã thay đổi vị trí hoặc tên bài tập. Bạn trỏ đến vị trí của bài tập bằng dòng lệnh sau:\n",
    "```\n",
    "python check_score.py -filepath <PATH_ĐẾN_FILE_.IPYNB_BÀI_LÀM_CỦA_BẠN>\n",
    "```\n",
    "Sau khi chạy dòng lệnh trên, vui lòng điền `username`, bạn sẽ nhận được kết quả trả về cho bài làm của bạn.  \n",
    "\n",
    "**Lưu ý:** Bạn có thể kiểm tra điểm bằng cách chạy file `check_score.py` bao nhiêu lần bạn muốn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Điểm số\n",
    "Điểm tối đa cho bài tập này là **30 điểm**. Mỗi bài tập nhỏ TO_DO_x nếu làm đúng hoàn toàn sẽ được **10 điểm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Đọc thêm) Dựa vào cách chia dữ liệu ở mỗi bước, có nhiều cách khác nhau để huấn luyện một Mô hình:\n",
    "\n",
    "##### 1) (Full) batch gradient descent: đưa toàn bộ X và Y vào để train:\n",
    "\n",
    "Với cách 1, do đưa toàn bộ batch vào nên gradient ở mỗi vòng lặp ổn định. Cách này được khuyến khích sử dụng khi hàm cost của mình biết rõ là convex (không có nhiều hơn 1 điểm tối ưu cục bộ). Tuy nhiên, đối với những hàm phức tạp, thì cách 1 có thể ko bao giờ đạt tối ưu toàn cục được.\n",
    "\n",
    "##### 2) Stochastic gradient descent: đưa từng cặp (x, y) trong data X, Y vào để train :\n",
    "\n",
    "Đối với cách 2, do mình đưa vào từng cặp nên gradient ở mỗi vòng lặp sẽ rất nhiễu (noisy). Chính vì sự nhiễu này mà có trong qúa trình học, nó có thể giúp mô hình vượt qua được các điểm tối ưu cục bộ. Stochastic = random, thể hiện cho sự nhiễu.\n",
    "##### 3) Mini-batch gradient descent: bốc 1 lượng nhiều hơn 1 mẫu từ X, Y để train.\n",
    "\n",
    "Cách 3 là sự kết hợp giữa 1 và 2, cũng là cách dùng nhiều nhất trong deep learning. Trong các bài tới sẽ đề cập sau.\n",
    "\n",
    "Còn về bài tập thì thực ra hàm error của mình hoàn toàn convex nên dùng cách 1 hay 2 đều được. Nhưng cách 2 sẽ lâu hơn. Bạn có thể sửa code lại để kiểm tra thử."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nhập thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "tf.__version__ # '2.x'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected original features are ['NOX' 'RM']\n"
     ]
    }
   ],
   "source": [
    "from utils_function import load_Boston_housing_data\n",
    "import numpy as np\n",
    "raw_train_X, test_X, train_Y, test_Y = load_Boston_housing_data(feature_ind = [4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 6. Khai báo các biến và siêu tham số được sử dụng để huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo siêu tham số huấn luyện\n",
    "learning_rate = 0.005\n",
    "training_epochs = 10000\n",
    "display_step = 1000\n",
    "n_samples, dimension = raw_train_X.shape\n",
    "batch_size = n_samples # Full Batch Gradient Descent\n",
    "\n",
    "# Khai báo các tensors input, output bằng kiểu tf.constant trong tensorflow.\n",
    "train_X = tf.constant(raw_train_X, dtype=tf.float64)\n",
    "\n",
    "train_Y = tf.reshape(tensor=train_Y, shape=(-1, 1))\n",
    "train_Y = tf.constant(train_Y, dtype=tf.float64) \n",
    "\n",
    "# Khai báo các tensors trọng số bằng kiểu tf.Variable trong tensorflow.\n",
    "## Khởi tạo trọng số cho quá trình huấn luyện bằng np.random.normal\n",
    "W = tf.Variable(np.random.normal(size=(dimension, 1)), trainable=True) \n",
    "b = tf.Variable(np.random.normal(size=(1, 1)), trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 7. Xây dựng Mô hình Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 6. Xây dựng Mô hình Linear Regression bằng Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def tf_lr_hypothesis(X, w, b):\n",
    "    ''' TO_DO_6: Sử dụng các hàm số cơ bản trong Tensorflow (tf.add và tf.matmul), xây dựng Mô hình Linear Regression.\n",
    "    Input:\n",
    "        - X: kiểu tf.Tensor, X.shape = (num_points,num_feat). Là mảng đầu vào của tập dữ liệu huấn luyện.\n",
    "             Mảng này là mảng 2 chiều, có số hàng tương đương num_points (số lượng dữ liệu) và số cột tương ứng với num_feat.\n",
    "        - w: (tương đương với mảng Theta–) kiểu tf.Tensor, w.shape = (num_feat, 1). \n",
    "             Là mảng các trọng số của các đặc trưng (features) trong mô hình Linear Regression.\n",
    "             Mảng này là mảng 2 chiều với 1 cột duy nhất, có số hàng bằng num_feat (Số lượng features). \n",
    "        - b: (tương đương với theta_0) kiểu tf.Tensor. b.shape = (1,1)\n",
    "             Là bias (hệ số tự do) của mô hình Linear Regression.\n",
    "    Output: \n",
    "        - Y_hat: kiểu tf.Tensor, Y_hat.shape = (num_points,1). Là mảng đầu ra được dự đoán bởi mô hình, với đầu vào X.\n",
    "                 Mảng này là mảng 1 chiều, có kích thước num_points (số lượng dữ liệu).\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    Y_hat = tf.math.add(tf.tensordot(X,w,1), b)\n",
    "    ### END CODE HERE ###\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check cho TO_DO_6*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "# Khai báo giá trị để test.\n",
    "X_test = tf.constant(np.array([[4,3],[5,3.2]]), dtype = tf.float64)\n",
    "Y_hat_test = tf.constant(np.array([[12.0],[14.2]]), dtype = tf.float64)\n",
    "w_test = tf.Variable(np.array([[2.0],[1.0]]))\n",
    "b_test = tf.Variable(np.array([[1.0]]))\n",
    "\n",
    "# Kiểm tra hàm đã viết\n",
    "pred_Y_hat = tf_lr_hypothesis(X_test, w_test, b_test)\n",
    "assert tf.reduce_all( tf.abs(pred_Y_hat-Y_hat_test) < epsilon ), \"Chưa đúng!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 8. Tính loss của mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 7. Viết chương trình tính loss bằng Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def tf_mse_loss(Y_hat, Y):\n",
    "    ''' TO_DO_7: Với mô hình dự đoán ở trên, tính giá trị loss sử dụng các hàm số cơ bản trong Tensorflow (tf.pow, tf.reduce_sum,...)\n",
    "    Input:\n",
    "        - Y_hat: kiểu tf.Tensor, Y_hat.shape = (num_points,1). Là mảng đầu ra **được dự đoán bởi mô hình**.\n",
    "                 Mảng này là mảng 2 chiều với 1 cột duy nhất, có số hàng bằng num_feat (Số lượng features). \n",
    "        - Y: kiểu tf.Tensor, Y.shape = (num_points, 1). Là mảng đầu ra **thực tế của tập dữ liệu**.\n",
    "                 Mảng này là mảng 2 chiều với 1 cột duy nhất, có số hàng bằng num_feat (Số lượng features). \n",
    "    Output: \n",
    "        - loss: kiểu tf.Tensor, loss.shape = (). Là một giá trị duy nhất đại diện cho loss của mô hình, trên tập dữ liệu (X,Y).\n",
    "    '''    \n",
    "    ### START CODE HERE ###\n",
    "    loss = tf.math.scalar_mul((1/(2*len(Y))),tf.math.reduce_sum(tf.math.square(Y_hat-Y)))\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check cho TO_DO_7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "# Khai báo giá trị để test.\n",
    "Y_hat_test = tf.constant(np.array([[12.0],[14.2]]), dtype = tf.float64)\n",
    "Y_test = tf.constant(np.array([[11.0],[12.0]]), dtype = tf.float64)\n",
    "\n",
    "loss_test = tf.constant(1.46,dtype = tf.float64)\n",
    "\n",
    "# Kiểm tra hàm đã viết\n",
    "pred_loss_test = tf_mse_loss(Y_hat_test,Y_test)\n",
    "assert tf.abs(loss_test - pred_loss_test) < epsilon, \"Chưa đúng!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 9. Khởi tạo hàm tối ưu hoá cho quá trình huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 10. Huấn luyện mô hình - Tìm tham số tối ưu bằng Gradient Descent qua thư viện Tensorflow\n",
    "\n",
    "Sau khi đã có Mô hình, Hàm mất mát và Hàm số tối ưu Gradient Descent, ta thực hiện quá trình huấn luyện với số bước huấn luyện lặp lại là `training_epochs`. Quá trình huấn luyện sẽ lần lượt sử dụng các hàm số mà chúng ta đã định nghĩa ở bước trước đó:\n",
    "- `tf_lr_hypothesis`\n",
    "- `tf_mse_loss`\n",
    "- `optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | Loss: 25.945306847715713\n",
      "Epoch: 2000 | Loss: 24.193915339359013\n",
      "Epoch: 3000 | Loss: 22.860881650778122\n",
      "Epoch: 4000 | Loss: 21.84621240918372\n",
      "Epoch: 5000 | Loss: 21.073817021593662\n",
      "Epoch: 6000 | Loss: 20.4857937198227\n",
      "Epoch: 7000 | Loss: 20.03808153268077\n",
      "Epoch: 8000 | Loss: 19.697151658618335\n",
      "Epoch: 9000 | Loss: 19.437489768742743\n",
      "Epoch: 10000 | Loss: 19.239680167253038\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs): \n",
    "    # Với mỗi epoch, ta thực hiện quá trình sau.\n",
    "    \n",
    "    with tf.GradientTape() as tape: \n",
    "    # Thực hiện các bước tính toán hàm loss ở trong tf.GradientTape để lưu trữ giá trị gradient.\n",
    "        # 1. Tính giá trị dự đoán\n",
    "        Y_hat = tf_lr_hypothesis(train_X, W, b) \n",
    "        \n",
    "        # 2. Tính giá trị mất mát loss\n",
    "        mse_loss = tf_mse_loss(Y_hat, train_Y)\n",
    "        \n",
    "    # 3. Lấy giá trị gradient được tính toán tự động trong tf.GradientTape\n",
    "    grads = tape.gradient(mse_loss, [W, b])\n",
    "    \n",
    "    # 4. Dùng hàm tối ưu hoá để cập nhật trọng số mới.\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "    \n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print(\"Epoch:\", epoch + 1, \"| Loss:\", mse_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Tạo các đặc trưng mới cho dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bài 11. Tạo các đặc tính mới (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = raw_train_X[:,0].reshape((n_samples,1)) # Đặc trưng thứ nhất\n",
    "X2 = raw_train_X[:,1].reshape((n_samples,1)) # Đặc trưng thứ hai\n",
    "\n",
    "# Tạo các đặc trưng mới X1_sqr, sin_X2 và X1X2\n",
    "X1_sqr = (X1**2).reshape((n_samples,1))\n",
    "sin_X2 = (np.sin(X2)).reshape((n_samples,1))\n",
    "\n",
    "X1X2 = (X1*X2).reshape((n_samples,1))\n",
    "\n",
    "# Thêm các đặc trưng mới này vào tập đặc trưng của dữ liệu đầu vào. Sử dụng np.concatenate\n",
    "new_train_X = np.concatenate((X1,X2,X1_sqr,sin_X2,X1X2),axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bài 12. Khai báo các biến, siêu tham số và hàm optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các tensors input, output bằng kiểu tf.constant trong tensorflow.\n",
    "new_train_X = tf.constant(new_train_X, dtype=tf.float64)\n",
    "train_Y = tf.reshape(tensor=train_Y, shape=(-1, 1))\n",
    "train_Y = tf.constant(train_Y, dtype=tf.float64) \n",
    "\n",
    "# Khai báo các tensors trọng số bằng kiểu tf.Variable trong tensorflow.\n",
    "_, new_dimension = new_train_X.shape\n",
    "W = tf.Variable(np.random.normal(size=(new_dimension, 1)), trainable=True) # create weights variable to train\n",
    "b = tf.Variable(np.random.normal(size=(1, 1)), trainable=True)\n",
    "\n",
    "# Khai báo các siêu tham số\n",
    "training_epochs = 10000\n",
    "display_step = 1000\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Khai báo hàm optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bài 13. Chạy chương trình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huấn luyện với dữ liệu có thêm các đặc trưng mới.\n",
      "Epoch: 1000 | Loss: 18.342822472145922\n",
      "Epoch: 2000 | Loss: 18.169327269998174\n",
      "Epoch: 3000 | Loss: 18.15608185642685\n",
      "Epoch: 4000 | Loss: 18.14981425960677\n",
      "Epoch: 5000 | Loss: 18.144283460764182\n",
      "Epoch: 6000 | Loss: 18.138858884613484\n",
      "Epoch: 7000 | Loss: 18.133467869526967\n",
      "Epoch: 8000 | Loss: 18.128101848998472\n",
      "Epoch: 9000 | Loss: 18.122759703910866\n",
      "Epoch: 10000 | Loss: 18.117441182249078\n"
     ]
    }
   ],
   "source": [
    "print(\"Huấn luyện với dữ liệu có thêm các đặc trưng mới.\")\n",
    "for epoch in range(training_epochs): \n",
    "    # Với mỗi epoch, ta thực hiện quá trình sau.\n",
    "    \n",
    "    with tf.GradientTape() as tape: \n",
    "    # Thực hiện các bước tính toán hàm loss ở trong tf.GradientTape để lưu trữ giá trị gradient.\n",
    "        # 1. Tính giá trị dự đoán\n",
    "        Y_hat = tf_lr_hypothesis(new_train_X, W, b)\n",
    "        \n",
    "        # 2. Tính giá trị mất mát loss\n",
    "        mse_loss = tf_mse_loss(Y_hat, train_Y)\n",
    "        \n",
    "    # 3. Lấy giá trị gradient được tính toán tự động trong tf.GradientTape\n",
    "    grads = tape.gradient(mse_loss, [W, b])\n",
    "    \n",
    "    # 4. Dùng hàm tối ưu hoá để cập nhật trọng số mới.\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "    \n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print(\"Epoch:\", epoch + 1, \"| Loss:\", mse_loss.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
